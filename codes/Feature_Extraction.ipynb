{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk, re, pprint\n",
    "import urllib\n",
    "import urllib.request\n",
    "import json\n",
    "import itertools\n",
    "import collections\n",
    "import operator\n",
    "from __future__ import division\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working on Reviews - Overall Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlcontext.read.json('./data/reviews_Cell_Phones_and_Accessories.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows with missing values\n",
    "df2 = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of complete rows\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.registerTempTable(\"df2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Only retain products with more than 4000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sqlcontext.sql(\"SELECT * FROM df2 WHERE asin in (SELECT asin from rating WHERE count >4000)\")\n",
    "ratings.registerTempTable(\"df3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Cleaning of Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(review):\n",
    "    \n",
    "    # Removes non-english words (punctuations and numbers)\n",
    "    import re\n",
    "    cleaned_text = re.sub(\"[^a-zA-Z\\'\\.]\", \" \", str(review))\n",
    "    \n",
    "    # Covert everything to lower case\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    # Remove the word 'quot'\n",
    "    cleaned_text = cleaned_text.replace(\"quot\", \"\")\n",
    "    \n",
    "    # Remove any extra white space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = ratings.rdd\\\n",
    "    .map(lambda x: (x['asin'], x['helpful'], x['overall'], clean_text(x['reviewText']), x['reviewTime'], x['reviewerID'], x['reviewerName'], x['summary'], x['unixReviewTime'])).toDF(['asin','helpful','overall','reviewText_clean','reviewTime','reviewerID','reviewerName', 'summary','unixReviewTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Subjective and Objective Sentence Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity(data):\n",
    "    Sub = []\n",
    "    l2 = []\n",
    "    l = data.split('.')\n",
    "    for i in range(len(l)):\n",
    "        blob = TextBlob(l[i])\n",
    "        for sentence in blob.sentences: \n",
    "            Sub = sentence.sentiment.subjectivity\n",
    "            if Sub > 0.2:\n",
    "                l2.append(l[i])\n",
    "    return '.'.join(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.rdd\\\n",
    "    .map(lambda x: (x['asin'], x['helpful'], x['overall'], subjectivity(x['reviewText_clean']), x['reviewTime'], x['reviewerID'], x['reviewerName'], x['summary'], x['unixReviewTime'])).toDF(['asin','helpful','overall','reviewText_sub','reviewTime','reviewerID','reviewerName', 'summary','unixReviewTime'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(text):\n",
    "    text_pos = nltk.word_tokenize(text)\n",
    "    text_pos = nltk.pos_tag(text_pos)\n",
    "    return text_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.rdd\\\n",
    "    .map(lambda x: (x['asin'], x['helpful'], x['overall'], pos(x['reviewText_sub']), x['reviewTime'], x['reviewerID'], x['reviewerName'], x['summary'], x['unixReviewTime'])).toDF(['asin','helpful','overall','reviewText_pos','reviewTime','reviewerID','reviewerName', 'summary','unixReviewTime'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Filtering Based on Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(text):\n",
    "    li = []\n",
    "    for i in range(len(text)-1):\n",
    "        if i == len(text) - 2:\n",
    "            if text[i][1] == \"JJ\" and (text[i+1][1] == \"NN\" or text[i+1] == \"NNS\"):\n",
    "                li.append(text[i])\n",
    "                li.append(text[i+1])\n",
    "            elif (text[i][1] == \"RB\" or text[i][1] == \"RBR\" or text[i][1] == \"RBS\") and (text[i+1][1] == \"VB\" or text[i+1][1] == \"VBN\" or text[i+1][1] == \"VBD\" or text[i+1][1] == \"VBG\"):\n",
    "                li.append(text[i])\n",
    "                li.append(text[i+1])\n",
    "            \n",
    "        elif text[i][1] == \"JJ\" and (text[i+1][1] == \"NN\" or text[i+1] == \"NNS\"):\n",
    "            li.append(text[i])\n",
    "            li.append(text[i+1])\n",
    "        elif (text[i][1] == \"RB\" or text[i][1] == \"RBR\" or text[i][1] == \"RBS\") and (text[i+1][1] == \"JJ\") and (text[i+2][1] != \"NN\" and text[i+2][1] != \"NNS\"):\n",
    "            li.append(text[i])\n",
    "            li.append(text[i+1])\n",
    "            li.append(text[i+2])\n",
    "        elif (text[i][1] == \"JJ\" ) and (text[i+1][1] == \"JJ\") and (text[i+2][1] != \"NN\" and text[i+2][1] != \"NNS\"):\n",
    "            li.append(text[i])\n",
    "            li.append(text[i+1])\n",
    "            li.append(text[i+2])\n",
    "        elif (text[i][1] == \"NN\" or text[i][1] == \"NNS\") and (text[i+1][1] == \"JJ\") and (text[i+2][1] != \"NN\" and text[i+2][1] != \"NNS\"):\n",
    "            li.append(text[i])\n",
    "            li.append(text[i+1])\n",
    "            li.append(text[i+2])\n",
    "        elif (text[i][1] == \"RB\" or text[i][1] == \"RBR\" or text[i][1] == \"RBS\") and (text[i+1][1] == \"VB\" or text[i+1][1] == \"VBN\" or text[i+1][1] == \"VBD\" or text[i+1][1] == \"VBG\"):\n",
    "            li.append(text[i])\n",
    "            li.append(text[i+1])\n",
    "\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.rdd\\\n",
    "    .map(lambda x: (x['asin'], x['helpful'], x['overall'], tagging(x['reviewText_pos']), x['reviewTime'], x['reviewerID'], x['reviewerName'], x['summary'], x['unixReviewTime'])).toDF(['asin','helpful','overall','reviewText_tag','reviewTime','reviewerID','reviewerName', 'summary','unixReviewTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.registerTempTable(\"df6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Pointwise Mutual Information - Information Retrieval Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "newlist=[]\n",
    "newlist1=[]\n",
    "ct=0\n",
    "\n",
    "\n",
    "def hits(word1,word2=\"\"): #\n",
    "    query = \"https://www.googleapis.com/customsearch/v1?key=AIzaSyCSggcnYzxwb67eSbwTa-cMmFglqqSJFOY&cx=007528943775727266505:5j7gzqbpr5g&q=\"\n",
    "    if word2 == \"\":\n",
    "#        results = urllib.request.urlopen(query % word1)\n",
    "        with urllib.request.urlopen(query + word1) as url:\n",
    "            resuts = url.read()\n",
    "    else:\n",
    "#        results = urllib.request.urlopen(query % word1+\" \"+\"AROUND(10)\"+\" \"+word2)\n",
    "        with urllib.request.urlopen(query + word1+\"%20\"+word2) as url:\n",
    "            resuts = url.read()\n",
    "    json_res = json.loads(results.read())\n",
    "    google_hits=int(json_res['responseData']['cursor']['estimatedResultCount'])\n",
    "    return google_hits\n",
    "\n",
    "\n",
    "def so(phrase):\n",
    "    num = hits(phrase,\"excellent\")\n",
    "    #print num\n",
    "    den = hits(phrase,\"poor\")\n",
    "    #print den\n",
    "    ratio = num / den\n",
    "    #print ratio\n",
    "    sop = log(ratio)\n",
    "    return sop\n",
    "\n",
    "list_first = [\"RB\",\"RBR\",\"RBS\"]\n",
    "list_second = [\"VB\",\"VBD\",\"VBN\",\"VBG\"]\n",
    "list_combn = itertools.product(list_first,list_second)\n",
    "\n",
    "      \n",
    "\n",
    "def check(newl,spl1):\n",
    "    print(newl)\n",
    "    print(spl1)\n",
    "    for k in range(0,len(newl)):\n",
    "        if(k!=len(newl)-1):\n",
    "            list_new=[]\n",
    "            list_new.append(newl[k])\n",
    "            list_new.append(newl[k+1])\n",
    "            list_new = tuple(list_new)\n",
    "        \n",
    "            if( newl[k]==\"JJ\" and newl[k+1]==\"JJ\" and newl[k+2]!=\"NN\" and newl[k+2]!=\"NNS\"):\n",
    "                return \"\".join(spl1[k])+\" \"+\"\".join(spl1[k+1])\n",
    "                \n",
    "            if( newl[k]==\"JJ\" and newl[k+1]==\"NN\" ) or ( newl[k]==\"JJ\" and newl[k+1]==\"NNS\" ):\n",
    "                return \"\".join(spl1[k])+\" \"+\"\".join(spl1[k+1])\n",
    "                \n",
    "            if( newl[k]==\"NN\" and newl[k+1]==\"JJ\" and newl[k+2]!=\"NN\" and newl[k+2]!=\"NNS\") or ( newl[k]==\"NNS\" and newl[k+1]==\"JJ\" and newl[k+2]!=\"NN\" and newl[k+2]!=\"NNS\"):\n",
    "                return \"\".join(spl1[k])+\" \"+\"\".join(spl1[k+1])\n",
    "                \n",
    "            if( newl[k]==\"RB\" and newl[k+1]==\"JJ\" and newl[k+2]!=\"NN\" and newl[k+2]!=\"NNS\") or ( newl[k]==\"RBR\" and newl[k+1]==\"JJ\" and newl[k+2]!=\"NN\" and newl[k+2]!=\"NNS\") or ( newl[k]==\"RBS\" and newl[k+1]==\"JJ\" and newl[k+2]!=\"NN\" and newl[k+2]!=\"NNS\"):\n",
    "                return \"\".join(spl1[k])+\" \"+\"\".join(spl1[k+1])\n",
    "                \n",
    "            for iter in list_combn:\n",
    "                if(list_new == iter):\n",
    "                    return \"\".join(spl1[k])+\" \"+\"\".join(spl1[k+1])\n",
    "            \n",
    "            \n",
    "\n",
    "def text_pos(raw):\n",
    "    global list,newlist,newlist1,ct\n",
    "    print(\"raw input:\",raw)\n",
    "    spl=raw.split()\n",
    "    print(\"\\n\")\n",
    "    print(\"split version of input:\",spl)\n",
    "    pos=nltk.pos_tag(spl)\n",
    "    print(\"\\n\")\n",
    "    print(\"POS tagged text:\",\"\")\n",
    "    for iter in pos:\n",
    "        print(iter,\"\")\n",
    "    for i in range(0,len(pos)):\n",
    "        if(i!=len(pos)-1):\n",
    "            list.append(pos[i])\n",
    "            list.append(pos[i+1])\n",
    "            t1 = list[0]\n",
    "            t2 = list[1]\n",
    "            newlist.append(t1[1])\n",
    "            newlist.append(t2[1])\n",
    "            list=[]\n",
    "    print(\"\\n\")\n",
    "    print(\"Extracting the tags alone:\",\"\")\n",
    "    print(newlist)\n",
    "    for j in range(0,len(newlist)):\n",
    "        if((j%2!=0) and (j!=len(newlist)-1)):\n",
    "            newlist[j]=0\n",
    "            \n",
    "    newlist = [x for x in newlist if x != 0]\n",
    "    print(\"Checking whether the tags conform to the required pattern...\")\n",
    "    print(\"\\n\")\n",
    "    print(spl)\n",
    "    print(newlist)\n",
    "    print(\"The extracted two-word phrases which satisfy the required pattern are:\")\n",
    "    strr1=check(newlist,spl)\n",
    "    return strr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = text_pos(\"Nokia is a amazing phone\")\n",
    "print(strr)\n",
    "x = so(strr)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. Semantic Orientation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def so(sentence):\n",
    "    score_final = analyser.polarity_scores(sentence)\n",
    "    return score_final['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb(sentence):\n",
    "    score = 0\n",
    "    blob = TextBlob(sentence)\n",
    "    for sentence in blob.sentences:\n",
    "        score = score + (sentence.sentiment.polarity)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df4.rdd\\\n",
    "    .map(lambda x: (x['asin'], x['helpful'], x['overall'], so(x['reviewText_sub']), tb(x['reviewText_sub']), x['reviewTime'], x['reviewerID'], x['reviewerName'], x['summary'], x['unixReviewTime'])).toDF(['asin','helpful','overall','reviewText_so', 'reviewText_tb','reviewTime','reviewerID','reviewerName', 'summary','unixReviewTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. Overall Rating\n",
    "This was found via two ways of semactic orientation - using VADER and using Textblob packages as shown in 3.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.registerTempTable(\"df7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_rat = sqlcontext.sql(\"SELECT reviewText_tb, reviewText_so FROM df7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_rat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_asin = overall_rat.select(\"asin\").rdd.flatMap(lambda x: x).collect()\n",
    "overall_rating = overall_rat.select(\"overall\").rdd.flatMap(lambda x: x).collect()\n",
    "overall_so = overall_rat.select(\"reviewText_so\").rdd.flatMap(lambda x: x).collect()\n",
    "overall_tb = overall_rat.select(\"reviewText_tb\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SO = pd.DataFrame(overall_so)\n",
    "SO.columns = {'SO'}\n",
    "ID = pd.DataFrame(overall_asin)\n",
    "ID.columns = {'ID'}\n",
    "TB = pd.DataFrame(overall_tb)\n",
    "TB.columns = {'TB'}\n",
    "Rating = pd.DataFrame(overall_rating)\n",
    "Rating.columns = {'Rating'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. Normalizing the Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([ID, SO, TB, Rating], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overall_so'] = df['SO']*df['Rating']\n",
    "df['overall_tb'] = df['TB']*df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = df['overall_so'].min()\n",
    "maxi = df['overall_so'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = np.array(df['overall_so'])\n",
    "review_final = (review - mini)*5/(maxi - mini)\n",
    "review_final.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overall_final_so'] = pd.DataFrame(review_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['ID','Rating', 'overall_final_so']].groupby(['ID']).mean()\n",
    "df2 = df2.reset_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Working on Reviews - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sqlcontext.sql(\"Select reviewText_tag from df6\")\n",
    "d = sqlcontext.sql(\"Select asin from df6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext = c.select(\"reviewText_tag\").rdd.flatMap(lambda x:x).collect()\n",
    "asin = d.select(\"asin\").collect()\n",
    "asin = [x.asin for x in asin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['asin','reviewtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['asin'] = asin\n",
    "data['reviewtext'] = reviewtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for i in data['asin'].unique():\n",
    "    mapping[i] = [data['reviewtext'][j] for j in data[data['asin']==i].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for i in mapping.keys():\n",
    "    count.append(len(mapping[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for key,value in mapping.items():\n",
    "    count = {}\n",
    "    for i in value:\n",
    "        for j in i:\n",
    "            #print(i)\n",
    "            if j._2 == 'NN':\n",
    "                if j._1 not in count:\n",
    "                    count[j._1] = 1\n",
    "                else:\n",
    "                    count[j._1] += 1\n",
    "    count = sorted(count.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    values.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun = pd.DataFrame(columns=['asin','reviewtext_noun'])\n",
    "data_noun['asin'] = mapping.keys()\n",
    "data_noun['reviewtext_noun'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal(li):\n",
    "    val = []\n",
    "    common_words = ['power', 'port', 'ports','usb', 'charger', 'battery', 'fit', 'weight', 'size', 'charge', 'build', 'price', 'quality', 'display','case', 'cases','panel', 'plastic', 'warranty', 'charging', 'cable', 'cost', 'connection', 'phone', 'color', 'charges', 'cord', 'protect', 'protection', 'protects', 'protector', 'package', 'packaging', 'connectors', 'connector','glass', 'brand', 'shock', 'button', 'service', 'crack', 'cracks', 'scratch', 'install', 'installation', 'bubble', 'bubbles']\n",
    "    for i in li:\n",
    "        if i[0] in common_words:\n",
    "            val.append(i)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data_noun['reviewtext_noun'].map(removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun['reviewtext_noun'] = data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun.to_csv('noun.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_words = {}\n",
    "for key,value in mapping.items():\n",
    "    for j in range(len(value)):\n",
    "        for k in range(len(value[j])):\n",
    "            value[j][k] = value[j][k]._1\n",
    "    mapping_words[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in mapping_words.items():\n",
    "    for j in range(len(value)):\n",
    "        value[j] = ' '.join(value[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(li):\n",
    "    copy_3 = []\n",
    "    for i in range(len(li)):\n",
    "        if len(li[i].split(' ')) > 2:\n",
    "            a = word_tokenize(li[i])\n",
    "            b = ngrams(a,3)\n",
    "            copy_3.append(list(b))\n",
    "        else:\n",
    "            c = word_tokenize(li[i])\n",
    "            d = ngrams(c,2)\n",
    "            copy_3.append(list(d))\n",
    "    return copy_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in mapping_words.items():\n",
    "    mapping_words[key] = ngram(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in mapping_words.items():\n",
    "    val = []\n",
    "    for j in value:\n",
    "        for k in j:\n",
    "            val.append(k)\n",
    "    mapping_words[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(asin,text):\n",
    "    noun_mapping = {}\n",
    "    for i in text:\n",
    "        for value in mapping_words[asin]:\n",
    "            if i[0] in value:\n",
    "                if i[0] not in noun_mapping:\n",
    "                    noun_mapping[i[0]] = [value]\n",
    "                else:\n",
    "                    noun_mapping[i[0]].append(value)\n",
    "    return noun_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = []\n",
    "for i in range(len(data_noun)):\n",
    "    maps.append(nouns(data_noun.iloc[i,0],data_noun.iloc[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence):\n",
    "    score_final = analyser.polarity_scores(sentence)\n",
    "    return -score_final['neg'] if score_final['compound'] < 0 else score_final['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for i in maps:\n",
    "    noun_score = {}\n",
    "    for key,value in i.items():\n",
    "        scores = 0\n",
    "        for j in value:\n",
    "            score = sentiment(' '.join(j))\n",
    "            scores += score\n",
    "        noun_score[key] = scores/len(value)\n",
    "    noun_score = sorted(noun_score.items(),key=operator.itemgetter(1))\n",
    "    final.append(noun_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun['final_featur_scores'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final)):\n",
    "    for j in range(len(final[i])):\n",
    "        final[i][j] = list(final[i][j]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_new)):\n",
    "    for j in range(len(data_new[i])):\n",
    "        data_new[i][j] = list(data_new[i][j]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = list(data_new)\n",
    "final = list(data_noun['final_featur_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_count_sort(first,second):\n",
    "    li = []\n",
    "    for i in range(len(first)):\n",
    "        for j in range(len(second)):\n",
    "             if first[i][0] == second[j][0]:\n",
    "                    li.append([first[i][0],second[j][1],first[i][1]])\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_map = []\n",
    "for i in range(len(new)):\n",
    "    final_map.append(final_count_sort(new[i],final[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun['final_features'] = final_map\n",
    "data_noun['review_count'] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Extract Top 5 Positive and Negative Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg(text):\n",
    "    pos = {'pos':[],'neg':[]}\n",
    "    for i in text:\n",
    "        if i[1] >= 0.1 and len(pos['pos']) <= 4:\n",
    "            pos['pos'].append([i[0],i[2]])\n",
    "        elif i[1] < 0.1 and len(pos['neg']) <= 4:\n",
    "            pos['neg'].append([i[0],i[2]])\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_noun)):\n",
    "    data_noun['pos_neg'] = data_noun['final_features'].map(pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for i in range(len(data_noun)):\n",
    "    pos.append(data_noun['pos_neg'][i]['pos'])\n",
    "    neg.append(data_noun['pos_neg'][i]['neg'])\n",
    "\n",
    "pos_count = []\n",
    "neg_count = []\n",
    "for i in range(len(data_noun)):\n",
    "    for j in range(5):\n",
    "        pos_count.append(pos[i][j][1])\n",
    "        neg_count.append(neg[i][j][1])\n",
    "\n",
    "pos_name = []\n",
    "neg_name = []\n",
    "for i in range(len(data_noun)):\n",
    "    for j in range(5):\n",
    "        pos_name.append(pos[i][j][0])\n",
    "        neg_name.append(neg[i][j][0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = np.array(pos_count)\n",
    "pos_count = pos_count.reshape(20,5)\n",
    "neg_count = np.array(neg_count)\n",
    "neg_count = neg_count.reshape(20,5)\n",
    "\n",
    "pos_name = np.array(pos_name)\n",
    "pos_name = pos_name.reshape(20,5)\n",
    "neg_name = np.array(neg_name)\n",
    "neg_name = neg_name.reshape(20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(count)):\n",
    "    pos_count[i] = pos_count[i]*100/count[i]\n",
    "    neg_count[i] = neg_count[i]*100/count[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_val = []\n",
    "neg_val = []\n",
    "for i in range(20):\n",
    "    pos_val.append(list(pos_name[i]))\n",
    "    neg_val.append(list(neg_name[i]))\n",
    "\n",
    "pos_cval = []\n",
    "neg_cval = []\n",
    "for i in range(20):\n",
    "    pos_cval.append(list(pos_count[i]))\n",
    "    neg_cval.append(list(neg_count[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['asin', 'pos', 'count'])\n",
    "df['pos'] = pos_val\n",
    "df['count'] = pos_cval\n",
    "df['asin'] = data_noun['asin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns = ['asin', 'neg', 'count'])\n",
    "df1['neg'] = neg_val\n",
    "df1['count'] = neg_cval\n",
    "df1['asin'] = data_noun['asin']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
